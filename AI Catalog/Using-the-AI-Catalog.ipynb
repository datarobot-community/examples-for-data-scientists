{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the AI Catalog\n",
    "\n",
    "This code example provides instruction on how to create and share datasets in the AI Catalog and then use them to create projects and generate predictions.\n",
    "\n",
    "Download this notebook from the [code examples home page](index).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "* Python 3.7+\n",
    "* DataRobot API version 2.21+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datarobot as dr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to DataRobot\n",
    "\n",
    "Read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To connect to a Zepl notebook:\n",
    "# dr.Client(token=z.getDatasource(\"datarobot_api\")['token'] , endpoint='https://app.datarobot.com/api/v2')\n",
    "\n",
    "# To connect to a Jupyter notebook:\n",
    "dr.Client(config_path = '/Users/nathan.goudreault/.config/datarobot/drconfig.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataset or a data source\n",
    "\n",
    "From the following commands, use the code that corresponds to your dataset or data source type to upload it to the AI Catalog. You can also use commands to [connect to a database](#connecting-to-a-database). Be sure to indiciate the correct path to your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = 'data.csv' # Provide your dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a local file\n",
    "dataset = dr.Dataset.create_from_file(file_path=path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a file object\n",
    "with open(path_to_data, 'rb') as f:\n",
    "    dataset = dr.Dataset.create_from_file(filelike=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_to_data)\n",
    "df_lst = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a pandas data frame\n",
    "dataset = dr.Dataset.create_from_in_memory_data(data_frame=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a list of dictionaries representing rows of data\n",
    "dataset = dr.Dataset.create_from_in_memory_data(records=df_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on CSV data from a URL\n",
    "dataset = dr.Dataset.create_from_url(url='https://data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a driver\n",
    "ms_sql_driver = [drv for drv in dr.DataDriver.list() if drv.class_name == 'com.microsoft.sqlserver.jdbc.SQLServerDriver'][-1]\n",
    "\n",
    "# Create a data store\n",
    "datastore = dr.DataStore.create(data_store_type='jdbc', \n",
    "                                canonical_name='Demo DB', \n",
    "                                driver_id=ms_sql_driver.id, \n",
    "                                jdbc_url=creds['jdbc_url'])\n",
    "\n",
    "# Create a data source based on a query\n",
    "query = \"select * from db.schema.table\"\n",
    "params = dr.DataSourceParameters(data_store_id=datastore.id, \n",
    "                                 query=query)\n",
    "\n",
    "datasource = dr.DataSource.create(data_source_type='jdbc', \n",
    "                                  canonical_name='datasource_query', \n",
    "                                  params=params)\n",
    "\n",
    "# Create a data source based on a table\n",
    "params = dr.DataSourceParameters(data_store_id=datastore.id, \n",
    "                                 schema='schema',\n",
    "                                 table='table')\n",
    "\n",
    "datasource = dr.DataSource.create(data_source_type='jdbc', \n",
    "                                  canonical_name='datasource_table', \n",
    "                                  params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Share a dataset or data source\n",
    "\n",
    "Use the following command to specify a list of users to share data with and assign them a role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ['user@domain.com']\n",
    "role = dr.enums.SHARING_ROLE.READ_ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To share via an API call:\n",
    "data = {'data': [{'username': user, 'role': role} for user in users]}\n",
    "sharing_resp = dr_rest_call(f'/api/v2/datasets/{dataset.id}/accessControl', requests.patch, payload=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To share a data source with Python:\n",
    "access_lst = [dr.SharingAccess(username=user, role=role) for user in users]\n",
    "datasource.share(access_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a project from a dataset\n",
    "dr.Project.create_from_dataset(dataset_id=dataset.id, \n",
    "                               project_name=dataset.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a project from a data source\n",
    "dr.Project.create_from_data_source(data_source_id=datasource.id, \n",
    "                                   username=creds['db_user'], \n",
    "                                   password=creds['db_pass'], \n",
    "                                   project_name=datasource.canonical_name\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a dataset to generate batch predictions\n",
    "\n",
    "You can use a dataset to generate batch predictions for a deployment. Before proceeding, select a deployment and obtain its [deployment ID](https://docs.datarobot.com/en/docs/predictions/predapi/dep-pred.html#predictions-for-deployments). Additionally, provide the dataset ID (obtained from the AI Catalog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_id = 'deployment id'\n",
    "dataset_id = 'dataset id'\n",
    "\n",
    "# Prepare the parameters to run a batch prediction job\n",
    "data = {'deploymentId': deployment_id,\n",
    "        'passthroughColumnsSet': 'all',\n",
    "        'intakeSettings': \n",
    "            {'type': 'dataset',\n",
    "             'datasetId': dataset_id},\n",
    "        'outputSettings':\n",
    "            {'type': 'localFile', \n",
    "            }\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a batch prediction job\n",
    "batch_pred_resp = dr_rest_call('/api/v2/batchPredictions', requests.post, payload=data)\n",
    "\n",
    "# Retrieve the job ID and its object\n",
    "batch_pred_job_id = batch_pred_resp.json()['id']\n",
    "batch_pred_job = dr.BatchPredictionJob.get(batch_pred_job_id)\n",
    "\n",
    "# Once run, wait for the job to complete and for the results to write\n",
    "batch_pred_job.wait_for_completion()\n",
    "with open('data/predictions.csv', 'wb') as f:\n",
    "    batch_pred_job.download(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
