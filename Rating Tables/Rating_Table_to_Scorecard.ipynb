{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating Table to Scorecard\n",
    "**Authors**: Vinay Wunnava\n",
    "\n",
    "In this tutorial, you are going to learn how to take a rating table from a `Generalized Additive 2 Model`, also known as `GA2M` and transform it into a scorecard.\n",
    "\n",
    "**There are a few things that you need for this exercise:**\n",
    "\n",
    "1. Your DataRobot API Key\n",
    "2. A trained `Generalized Additive 2 Model` (on any project, with any dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinay.wunnava/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datarobot as dr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to DataRobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datarobot.rest.RESTClientObject at 0x7f9930a74160>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr.Client(token = 'YOUR_API_TOKEN',\n",
    "          endpoint = 'YOUR_ENDPOINT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Project and Model ID's.\n",
    "\n",
    "The project and model ID's can be found in the url (when you use the UI to navigate). Make sure that the GA2M model does not have **any text features as input**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = 'YOUR_PROJECT_ID'\n",
    "mid = 'YOUR_MODEL_ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions that download and transform the rating table to scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_rating_table(pid, mid):\n",
    "    \"\"\" Download the rating table corresponding to the pid and mid\n",
    "    \"\"\"\n",
    "    project = dr.Project.get(pid)\n",
    "    rating_tables = rating_tables = project.get_rating_tables()\n",
    "    rating_table = [rt for rt in rating_tables if rt.model_id == mid][0]\n",
    "    filepath = './my_rating_table_' + mid + '.csv'\n",
    "    rating_table.download('./my_rating_table_' + mid + '.csv')\n",
    "    return filepath\n",
    "\n",
    "def csv_after_emptylines(filepath, bl_group_n=1, dtype=str):\n",
    "    \"\"\" Read a .CSV into a Pandas DataFrame, but only after at least one blank line has been skipped.\n",
    "    bl_group_n is the expected number of distinct blocks of blank lines (of any number of rows each) to skip before reading data.\n",
    "    NB: E.g. pd.read_csv(filepath, skiprows=[0, 1, 2]) works if you know the number of rows to be skipped. Use this function if you have a variable / unknown number of filled rows (to be skipped / ignored) before the empty rows.\n",
    "    \"\"\"\n",
    "    with open(filepath, newline='') as f:\n",
    "        blank_lines = 0\n",
    "        bl_groups = 0\n",
    "        contents = []\n",
    "        headers = None\n",
    "        r = csv.reader(f)\n",
    "        for i, l in enumerate(r):\n",
    "            if bl_groups < bl_group_n:\n",
    "                if not l:\n",
    "                    blank_lines += 1\n",
    "                    continue\n",
    "                if blank_lines == 0:\n",
    "                    continue\n",
    "                bl_groups += 1\n",
    "                blank_lines = 0\n",
    "                headers = l\n",
    "                continue\n",
    "            contents.append(l)\n",
    "        return pd.DataFrame(data=contents, columns=headers, dtype=dtype)\n",
    "\n",
    "def csv_until_emptyline(filepath, dtype=str):\n",
    "    \"\"\" Read a .CSV into a Pandas DataFrame until a blank line is found, then stop.\n",
    "    \"\"\"\n",
    "    with open(filepath, newline='') as f:\n",
    "        contents = []\n",
    "        r = csv.reader(f)\n",
    "        for i, l in enumerate(r):\n",
    "            if not l:\n",
    "                break\n",
    "            if i == 0:\n",
    "                headers = l\n",
    "                continue\n",
    "            contents.append(l)\n",
    "        return pd.DataFrame(data=contents)\n",
    "\n",
    "def extract_intercept(filepath):\n",
    "    \"\"\" Read a .CSV into a Pandas DataFrame until a blank line is found, then stop.\n",
    "        Extract intercept value and return it\n",
    "    \"\"\" \n",
    "    df = csv_until_emptyline(filepath)\n",
    "    df.rename(columns={df.columns[0]: \"raw\" }, inplace = True)\n",
    "    df[['name','value']] = df['raw'].str.split(\":\",expand=True)\n",
    "    intercept = pd.to_numeric(df.loc[df.name == 'Intercept','value'].values[0])\n",
    "    return intercept\n",
    "\n",
    "def invert_coefficients(intercept, rating_table):\n",
    "    \"\"\" Inverting the sign of intercept and all the coefficients - this is to ensure that the high risk people are given low scores\n",
    "        Mathematically, we are modelling log of odds and the riskier profiles have high probability\n",
    "        When we negate the coefficients, it will mean the log of odds of non-risky profiles (- log(p/1-p) = log(1-p/p))\n",
    "    \"\"\"\n",
    "    intercept = - intercept\n",
    "    rating_table.loc[:,'Coefficient'] = - rating_table['Coefficient'].astype(float)\n",
    "    return intercept, rating_table\n",
    "\n",
    "def convert_rating_table_to_scores(intercept, rating_table, min_score=300, max_score=850):\n",
    "    rating_table['Rel_Coefficient'] = rating_table['Coefficient']\n",
    "    baseline = intercept\n",
    "    min_sum_coef = 0\n",
    "    max_sum_coef = 0\n",
    "    for feat in rating_table['Feature Name'].unique():\n",
    "        min_feat_coef = rating_table.loc[rating_table['Feature Name'] == feat]['Coefficient'].min()\n",
    "        print('Minimum coefficient for feature ' + feat + ' ' + str(min_feat_coef))\n",
    "        rating_table.loc[rating_table['Feature Name'] == feat,'Rel_Coefficient'] = rating_table['Coefficient'] - min_feat_coef\n",
    "        baseline += min_feat_coef\n",
    "        min_sum_coef = min_sum_coef + rating_table.loc[rating_table['Feature Name'] == feat]['Rel_Coefficient'].min()\n",
    "        max_sum_coef = max_sum_coef + rating_table.loc[rating_table['Feature Name'] == feat]['Rel_Coefficient'].max()\n",
    "\n",
    "    min_sum_coef = min_sum_coef + baseline\n",
    "    max_sum_coef = max_sum_coef + baseline\n",
    "    \n",
    "    scaling_factor = (max_score - min_score) / (max_sum_coef - min_sum_coef)\n",
    "    rating_table.loc[:,'Variable Score'] = rating_table['Rel_Coefficient'] * scaling_factor\n",
    "    baseline_score = ((baseline - min_sum_coef) * scaling_factor) + min_score\n",
    "    \n",
    "    return baseline_score, rating_table.drop(columns=['Coefficient','Rel_Coefficient']), min_sum_coef, max_sum_coef\n",
    "\n",
    "def get_scorecard(pid,mid, min_score=300, max_score=850):\n",
    "    \"\"\" Download rating table for a particular pid and mid and return scorecard\n",
    "    \"\"\"\n",
    "    filepath = download_rating_table(pid,mid)    \n",
    "    rating_table_raw = csv_after_emptylines(filepath)\n",
    "    intercept_raw = extract_intercept(filepath)\n",
    "    intercept, rating_table = invert_coefficients(intercept_raw, rating_table_raw)\n",
    "    intercept_score, scorecard, min_sum_coef, max_sum_coef = convert_rating_table_to_scores(intercept, rating_table, min_score, max_score)\n",
    "    \n",
    "    return intercept_score, scorecard, min_sum_coef, max_sum_coef\n",
    "\n",
    "def get_score_from_prob(prob_default, min_score, max_score, min_sum_coef, max_sum_coef):\n",
    "    \"\"\" Get score for probability of default and return score using the scorecard metrics - useful for threshold\n",
    "    \"\"\"\n",
    "    prob_non_default = 1 - prob_default\n",
    "    log_odds = np.log(prob_non_default / (1 - prob_non_default))\n",
    "    scaling_factor = (max_score - min_score) / (max_sum_coef - min_sum_coef)\n",
    "    score = ((log_odds - min_sum_coef) * scaling_factor) + min_score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Scorecard and Intercept Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum coefficient for feature annual_inc -0.7971001550635846\n",
      "Minimum coefficient for feature dti -0.18100027283235265\n",
      "Minimum coefficient for feature inq_last_6mths -0.3096886255271891\n",
      "Minimum coefficient for feature revol_util_percent -0.6561420159264604\n",
      "Minimum coefficient for feature ( inq_last_6mths & revol_util_percent ) -0.2953854956969044\n"
     ]
    }
   ],
   "source": [
    "intercept_score, scorecard, min_sum_coef, max_sum_coef = get_scorecard(pid,mid, min_score=300, max_score=850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Strength</th>\n",
       "      <th>Type</th>\n",
       "      <th>Transform1</th>\n",
       "      <th>Value1</th>\n",
       "      <th>Transform2</th>\n",
       "      <th>Value2</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Variable Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>annual_inc</td>\n",
       "      <td>0.300775009108807</td>\n",
       "      <td>NUM</td>\n",
       "      <td>Binning</td>\n",
       "      <td>(-inf, 13700.0]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>annual_inc</td>\n",
       "      <td>0.300775009108807</td>\n",
       "      <td>NUM</td>\n",
       "      <td>Binning</td>\n",
       "      <td>(13700.0, 20200.0]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>159.0</td>\n",
       "      <td>47.420770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>annual_inc</td>\n",
       "      <td>0.300775009108807</td>\n",
       "      <td>NUM</td>\n",
       "      <td>Binning</td>\n",
       "      <td>(20200.0, 21576.0]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>32.0</td>\n",
       "      <td>49.394504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>annual_inc</td>\n",
       "      <td>0.300775009108807</td>\n",
       "      <td>NUM</td>\n",
       "      <td>Binning</td>\n",
       "      <td>(21576.0, 22489.0]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>33.0</td>\n",
       "      <td>47.922292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>annual_inc</td>\n",
       "      <td>0.300775009108807</td>\n",
       "      <td>NUM</td>\n",
       "      <td>Binning</td>\n",
       "      <td>(22489.0, 24584.0]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>125.0</td>\n",
       "      <td>30.520414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>( inq_last_6mths &amp; revol_util_percent )</td>\n",
       "      <td>0.033830128907363956</td>\n",
       "      <td>2W-INT</td>\n",
       "      <td>Binning</td>\n",
       "      <td>(6, inf)</td>\n",
       "      <td>Binning</td>\n",
       "      <td>(89, 91]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.210374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>( inq_last_6mths &amp; revol_util_percent )</td>\n",
       "      <td>0.033830128907363956</td>\n",
       "      <td>2W-INT</td>\n",
       "      <td>Binning</td>\n",
       "      <td>(6, inf)</td>\n",
       "      <td>Binning</td>\n",
       "      <td>(91, 92]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.279859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>( inq_last_6mths &amp; revol_util_percent )</td>\n",
       "      <td>0.033830128907363956</td>\n",
       "      <td>2W-INT</td>\n",
       "      <td>Binning</td>\n",
       "      <td>(6, inf)</td>\n",
       "      <td>Binning</td>\n",
       "      <td>(92, 95]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.759709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>( inq_last_6mths &amp; revol_util_percent )</td>\n",
       "      <td>0.033830128907363956</td>\n",
       "      <td>2W-INT</td>\n",
       "      <td>Binning</td>\n",
       "      <td>(6, inf)</td>\n",
       "      <td>Binning</td>\n",
       "      <td>(95, inf)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.991202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>( inq_last_6mths &amp; revol_util_percent )</td>\n",
       "      <td>0.033830128907363956</td>\n",
       "      <td>2W-INT</td>\n",
       "      <td>Binning</td>\n",
       "      <td>Missing Value</td>\n",
       "      <td>Binning</td>\n",
       "      <td>Missing Value</td>\n",
       "      <td>5.0</td>\n",
       "      <td>34.268703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Feature Name      Feature Strength    Type  \\\n",
       "0                                 annual_inc     0.300775009108807     NUM   \n",
       "1                                 annual_inc     0.300775009108807     NUM   \n",
       "2                                 annual_inc     0.300775009108807     NUM   \n",
       "3                                 annual_inc     0.300775009108807     NUM   \n",
       "4                                 annual_inc     0.300775009108807     NUM   \n",
       "..                                       ...                   ...     ...   \n",
       "346  ( inq_last_6mths & revol_util_percent )  0.033830128907363956  2W-INT   \n",
       "347  ( inq_last_6mths & revol_util_percent )  0.033830128907363956  2W-INT   \n",
       "348  ( inq_last_6mths & revol_util_percent )  0.033830128907363956  2W-INT   \n",
       "349  ( inq_last_6mths & revol_util_percent )  0.033830128907363956  2W-INT   \n",
       "350  ( inq_last_6mths & revol_util_percent )  0.033830128907363956  2W-INT   \n",
       "\n",
       "    Transform1              Value1 Transform2         Value2 Weight  \\\n",
       "0      Binning     (-inf, 13700.0]                             57.0   \n",
       "1      Binning  (13700.0, 20200.0]                            159.0   \n",
       "2      Binning  (20200.0, 21576.0]                             32.0   \n",
       "3      Binning  (21576.0, 22489.0]                             33.0   \n",
       "4      Binning  (22489.0, 24584.0]                            125.0   \n",
       "..         ...                 ...        ...            ...    ...   \n",
       "346    Binning            (6, inf)    Binning       (89, 91]    1.0   \n",
       "347    Binning            (6, inf)    Binning       (91, 92]    0.0   \n",
       "348    Binning            (6, inf)    Binning       (92, 95]    2.0   \n",
       "349    Binning            (6, inf)    Binning      (95, inf)    0.0   \n",
       "350    Binning       Missing Value    Binning  Missing Value    5.0   \n",
       "\n",
       "     Variable Score  \n",
       "0          0.000000  \n",
       "1         47.420770  \n",
       "2         49.394504  \n",
       "3         47.922292  \n",
       "4         30.520414  \n",
       "..              ...  \n",
       "346       45.210374  \n",
       "347       43.279859  \n",
       "348       30.759709  \n",
       "349       31.991202  \n",
       "350       34.268703  \n",
       "\n",
       "[351 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(intercept_score)\n",
    "scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max probability of non-default: 0.9887734778925666\n",
      "Min probability of default: 0.011226522107433357\n"
     ]
    }
   ],
   "source": [
    "# Maximum probability of non-default based on rating table\n",
    "max_prob_non_default = 1 / (1 + np.exp(- max_sum_coef))\n",
    "print(f'Max probability of non-default: {max_prob_non_default}')\n",
    "# Minimum probability of default would be 1 - max_prob_non_default\n",
    "min_prob_default = 1 - max_prob_non_default\n",
    "print(f'Min probability of default: {min_prob_default}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min probability of non-default: 0.4345155432882876\n",
      "Max probability of default: 0.5654844567117123\n"
     ]
    }
   ],
   "source": [
    "# Minimum probability of non-default based on rating table\n",
    "min_prob_non_default = 1 / (1 + np.exp(- min_sum_coef))\n",
    "print(f'Min probability of non-default: {min_prob_non_default}')\n",
    "# Maximum probability of default would be 1 - max_prob_non_default\n",
    "max_prob_default = 1 - min_prob_non_default\n",
    "print(f'Max probability of default: {max_prob_default}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaling of the probabilities are <b>not</b> from \\[0, 1\\] to \\[300, 850\\] but from \\[min_prob_non_default, max_prob_non_default\\] to \\[300, 850\\]. The range of probabilities from the DR model correspond to the above values. The scaling applied here also correspond to the same range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576.1843879406667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert threshold or any probabilty of default to score (the probabilty should lie between the min and max of the probability of default)\n",
    "get_score_from_prob(0.1074, 300, 850, min_sum_coef, max_sum_coef) # f1 score opt - can be used as a threshold score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "850.2774648796886"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score_from_prob(0.0112, 300, 850, min_sum_coef, max_sum_coef) # Min prob default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300.0398687633396"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score_from_prob(0.5654, 300, 850, min_sum_coef, max_sum_coef) # Max prob default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
