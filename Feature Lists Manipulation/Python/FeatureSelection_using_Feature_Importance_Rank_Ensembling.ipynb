{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Feature Selection using Feature Importance Rank Ensembling (FIRE)\n",
    "\n",
    "<b>Authors:</b> Vitalii Peretiatko, Rajiv Shah\n",
    "\n",
    "<b>Scope:</b><br>\n",
    "This notebook shows the benefits of advanced feature selection that uses median rank agggregation of feature impacts across several models created during a run of DataRobot autopilot. \n",
    "\n",
    "<b>Background:</b>\n",
    " - Original blogpost: https://www.datarobot.com/blog/using-feature-importance-rank-ensembling-fire-for-advanced-feature-selection/ \n",
    " - Blogpost on advanced feature selection in R: https://community.datarobot.com/t5/resources/advanced-feature-selection-with-r/ta-p/5307\n",
    " - Instructions and logic of how to do advanced feature selection in Python: https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Feature%20Lists%20Manipulation/Python/Advanced%20Feature%20Selection.ipynb \n",
    " \n",
    "<b> Requirements </b><br>\n",
    "Python version >= 3.7.3<br>\n",
    "DataRobot API version >= 2.22.1.<br>\n",
    "Full documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dataset</b><br>\n",
    "We will be using Madelon dataset from this paper https://archive.ics.uci.edu/ml/datasets/Madelon <br>\n",
    "It can also be found here https://s3.amazonaws.com/datarobot_public_datasets/madelon_combined_80.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Import libraries and connect to DataRobot</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datarobot.rest.RESTClientObject at 0x121fcdc90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datarobot as dr\n",
    "\n",
    "#connect to DataRobot using the config file\n",
    "dr.Client(config_path='../../drconfig.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define the function for FIRE feature selection</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_rank_ensembling(project, \n",
    "                                          n_models=5, \n",
    "                                          metric=None, \n",
    "                                          by_partition='validation',\n",
    "                                          feature_list_name=None,\n",
    "                                          ratio=0.95,\n",
    "                                          model_search_params=None, \n",
    "                                          use_ranks=True,\n",
    "                                         ):   \n",
    "    \"\"\"\n",
    "    Function that implements the logic of Feature Selection using Feature Importance Rank Ensembling and restarts DR autopilot\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    project: DR project object, \n",
    "    n_models: int, get top N best models on the leaderboard to compute feature impact on. Default 5\n",
    "    metric: str, DR metric to check performance against. Default None. If Default, it will use DR project defined metric\n",
    "    by_partition: str, whether to use 'validation' or 'crossValidation' partition to get the best model on. Default 'validation'\n",
    "    feature_list_name: str, name of the feature list to start iterating from. Default None\n",
    "    ratio: float, ratio of total feature impact that new feature list will contain. Default 0.95\n",
    "    model_search_params: dict, dictonary of parameters to search the best model. See official DR python api docs. Default None\n",
    "    use_ranks: Boolean, True to use median rank aggregation or False to use total impact unnormalized. Default True\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    dr.Model object\n",
    "    \"\"\"\n",
    "    \n",
    "    models = get_best_models(project,\n",
    "                             metric=metric, \n",
    "                             by_partition=by_partition, \n",
    "                             start_featurelist_name=feature_list_name,\n",
    "                             model_search_params = model_search_params)\n",
    "    \n",
    "    models = models.values[:n_models]\n",
    "    \n",
    "    all_impact = pd.DataFrame()\n",
    "    \n",
    "    print(\"Request Feature Impact calculations\")\n",
    "    # first kick off all FI requests, let DR deal with parallelizing\n",
    "    for model in models:\n",
    "        try:\n",
    "            model.request_feature_impact()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for model in models:\n",
    "        # This can take some time to compute feature impact\n",
    "        feature_impact = pd.DataFrame(model.get_or_request_feature_impact(max_wait=60*15))#15min\n",
    "    \n",
    "        # Track model name and ID for bookkeeping purposes\n",
    "        feature_impact['model_type'] = model.model_type\n",
    "        feature_impact['model_id'] = model.id\n",
    "        # By sorting and re-indexing, the new index becomes our 'ranking'\n",
    "        feature_impact = feature_impact.sort_values(by='impactUnnormalized', ascending=False).reset_index(drop=True)\n",
    "        feature_impact['rank'] = feature_impact.index.values\n",
    "    \n",
    "        # Add to our master list of all models' feature ranks\n",
    "        all_impact = pd.concat([all_impact, feature_impact], ignore_index=True)\n",
    "     \n",
    "    #We need to get a threshold number of features to select.\n",
    "    #based on cumulative sum of impact\n",
    "    all_impact_agg = all_impact\\\n",
    "            .groupby('featureName')[['impactNormalized','impactUnnormalized']]\\\n",
    "            .sum()\\\n",
    "            .sort_values('impactUnnormalized', ascending=False)\\\n",
    "            .reset_index()\n",
    "    \n",
    "    #calculate cumulative feature impact and take first features that possess <ratio> of total impact\n",
    "    all_impact_agg['impactCumulative'] = all_impact_agg['impactUnnormalized'].cumsum()\n",
    "    total_impact = all_impact_agg['impactCumulative'].max() * ratio\n",
    "    tmp_fl = list(set(all_impact_agg[all_impact_agg.impactCumulative <= total_impact]['featureName'].values.tolist()))\n",
    "\n",
    "    #that will be a number of feature to use\n",
    "    n_feats = len(tmp_fl)\n",
    "    \n",
    "    if use_ranks:\n",
    "        #get top features based on median rank\n",
    "        top_ranked_feats = list(all_impact\n",
    "                                .groupby('featureName')\n",
    "                                .median()\n",
    "                                .sort_values('rank')\n",
    "                                .head(n_feats)\n",
    "                                .index\n",
    "                                .values)\n",
    "    else:\n",
    "        #otherwise get features based just on total unnormalized feature impact\n",
    "        top_ranked_feats = list(all_impact_agg.featureName.values[:n_feats])\n",
    "\n",
    "    ## Create a new featurelist\n",
    "    featurelist = project.create_modeling_featurelist(f'Reduced FL by Median Rank, top{n_feats}', top_ranked_feats)\n",
    "    featurelist_id = featurelist.id\n",
    "    ## Start autopilot\n",
    "    print('Starting AutoPilot on a reduced feature list')\n",
    "    project.start_autopilot(featurelist_id=featurelist_id, \n",
    "                            prepare_model_for_deployment=True,\n",
    "                            blend_best_models=False,\n",
    "                           )\n",
    "    wait_for_autopilot(project)\n",
    "    print('... AutoPilot is completed.')\n",
    "    #return the previous best model\n",
    "    return models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define the function to get the best model on the DR leaderboard</b>\n",
    "\n",
    "We do not want to pick all of the models. <br>\n",
    "We want to avoid using models trained on a higher (80%,100%) than 3rd stage of autopilot sample size.<br>\n",
    "We ignore Blenders and Frozen models.\n",
    "That means we mainly pick models trained on 64% percent of the data.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_models(project, \n",
    "                    metric=None, \n",
    "                    by_partition='validation',\n",
    "                    start_featurelist_name=None,\n",
    "                    model_search_params=None\n",
    "                   ):\n",
    "    '''\n",
    "    Gets pd.Series of DR model objects sorted by performance. Excludes blenders, frozend and on DR Reduced FL\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    project: DR project object\n",
    "    metric: str, metric to use for sorting models on lb, if None, default project metric will be used. Default None\n",
    "    by_partiton: boolean, whether to use 'validation' or 'crossValidation' partitioning. Default 'validation'\n",
    "    start_featurelist_name: str, initial featurelist name to get models on. Default None\n",
    "    model_search_params: dict to pass model search params. Default None\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    pd.Series of dr.Model objects, not blender, not frozen and not on DR Reduced Feature List\n",
    "    '''\n",
    "        \n",
    "    #list of metrics that get better as their value increases\n",
    "    desc_metric_list = ['AUC', 'Area Under PR Curve', 'Gini Norm', 'Kolmogorov-Smirnov', 'Max MCC', 'Rate@Top5%',\n",
    "                        'Rate@Top10%', 'Rate@TopTenth%', 'R Squared', 'FVE Gamma', 'FVE Poisson', 'FVE Tweedie',\n",
    "                        'Accuracy', 'Balanced Accuracy', 'FVE Multinomial', 'FVE Binomial'\n",
    "                       ]\n",
    "    \n",
    "    if not metric:\n",
    "        metric = project.metric\n",
    "        if 'Weighted' in metric:\n",
    "            desc_metric_list = ['Weighted ' + metric for metric in desc_metric_list]\n",
    "   \n",
    "    asc_flag = False if metric in desc_metric_list else True\n",
    "    \n",
    "    if project.is_datetime_partitioned:\n",
    "        assert by_partition in ['validation', 'backtesting', 'holdout'], \"Please specify correct partitioning, in datetime partitioned projects supported options are: 'validation', 'backtesting', 'holdout' \"\n",
    "        models_df =  pd.DataFrame(\n",
    "            [[model.metrics[metric]['validation'],\n",
    "              model.metrics[metric]['backtesting'],\n",
    "              model.model_category,\n",
    "              model.is_frozen,\n",
    "              model.featurelist_name,\n",
    "              model,\n",
    "             ] for model in project.get_datetime_models()],\n",
    "            columns=['validation', 'backtesting', 'category', 'is_frozen', 'featurelist_name', 'model']\n",
    "        ).sort_values([by_partition], ascending = asc_flag, na_position='last')\n",
    "    \n",
    "    else:\n",
    "        assert by_partition in ['validation', 'crossValidation', 'holdout'], \"Please specify correct partitioning, supported options are: 'validation', 'crossValidation', 'holdout' \"\n",
    "        models_df =  pd.DataFrame(\n",
    "            [[model.metrics[metric]['crossValidation'],\n",
    "              model.metrics[metric]['validation'],\n",
    "              model.model_category,\n",
    "              model.is_frozen,\n",
    "              model.featurelist_name,\n",
    "              model,\n",
    "         ] for model in project.get_models(with_metric = metric, search_params = model_search_params)],\n",
    "        columns=['crossValidation', 'validation', 'category', 'is_frozen', 'featurelist_name', 'model']\n",
    "    ).sort_values([by_partition], ascending = asc_flag, na_position='last')\n",
    "    \n",
    "    \n",
    "    if start_featurelist_name:\n",
    "        return models_df.loc[((models_df.category == 'model')&\\\n",
    "                              (models_df.is_frozen == False)&\\\n",
    "                              (models_df.featurelist_name == start_featurelist_name)\n",
    "                             ),'model']\n",
    "    else:\n",
    "        return models_df.loc[((models_df.category == 'model')&\\\n",
    "                              (models_df.is_frozen == False)&\\\n",
    "                              (models_df.featurelist_name.str.contains('DR Reduced Features M') == False)\n",
    "                             ),'model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Let's define the function to perform FIRE several times</b>\n",
    "<br>This function automatically executes FIRE feature selection algorithm on top N models.\n",
    "<br>Once the reduced feature list is created, it re-runs the Autopilot and waits until it finishes.\n",
    "<br>It then automatically sorts the models based on the project metric and computes DR feature impact. And iterates over again.\n",
    "<br>If the new feature list produces a model worse based on a metric, it will consume one \"life\". The algorithm will stop performing feature selection when no lifes are available. We start with 3 lifes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_feature_selection(project_id,\n",
    "                           start_featurelist_name=None,\n",
    "                           lifes=3,\n",
    "                           top_n_models=5,\n",
    "                           partition='validation',\n",
    "                           main_scoring_metric=None,\n",
    "                           initial_impact_reduction_ratio=0.95,\n",
    "                           best_model_search_params=None,\n",
    "                           use_ranks=True,\n",
    "                          ):\n",
    "    '''\n",
    "    Main function. Meant to get the optimal shortest feature list by repeating feature selection process until stop criteria is met.\n",
    "    Currently supports Binary, Regression, Multiclass, Datetime partitioned(OTV) and AutoTS DataRobot projects.\n",
    "    \n",
    "    Example usage:\n",
    "    >> import datarobot as dr    \n",
    "    >> dr.Client(config_path='PATH_TO_DR_CONFIG/drconfig.yaml')\n",
    "    TIP: set best_model_search_params = {'sample_pct__lte': 65} to avoid using models trained on a higher sample size than 3rd stage of autopilot, which is typically ~64% of the data\n",
    "    \n",
    "    >> main_feature_reduction('INSERT_PROJECT_ID',\n",
    "                              start_featurelist_name=None,\n",
    "                              lifes=3,\n",
    "                              top_n_models=5,\n",
    "                              partition='validation',\n",
    "                              main_scoring_metric=None,\n",
    "                              initial_impact_reduction_ratio=0.95,\n",
    "                              best_model_search_params=None,\n",
    "                              use_ranks=True)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    project_id: str, id of DR project, \n",
    "    start_featurelist_name: str, name of feature list to start iterating from. Default None\n",
    "    lifes: int, stopping criteria, if no best model produced after lifes iterations, stop feature reduction. Default 3\n",
    "    top_n_models: int, only for 'Rank Aggregation method', get top N best models on the leaderboard. Default 5\n",
    "    partition: str, whether to use 'validation','crossValidation' or 'backtesting' partition to get the best model on. Default 'validation'\n",
    "    main_scoring_metric: str, DR metric to check performance against, If None DR project metric will be used\n",
    "    initial_impact_reduction_ratio: float, ratio of total feature impact that new feature list will contain. Default 0.95\n",
    "    best_model_search_params: dict, dictonary of parameters to search the best model. See official DR python api docs. Default None\n",
    "    use_ranks: Boolean, True to use median rank aggregation or False to use total impact unnormalized. Default True\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    dr.Model object of the best model on the leaderboard\n",
    "    '''\n",
    "    project = dr.Project.get(project_id)\n",
    "        \n",
    "    ratio = initial_impact_reduction_ratio\n",
    "    assert ratio < 1, \"Please specify initial_impact_reduction_ratio < 1\"\n",
    "    \n",
    "    model_search_params = best_model_search_params\n",
    "    \n",
    "    runs = 0\n",
    "    #main function loop\n",
    "    while lifes > 0:\n",
    "        if runs > 0:\n",
    "            start_featurelist_name = None\n",
    "        try:    \n",
    "            best_model = feature_importance_rank_ensembling(project,\n",
    "                                                               n_models=top_n_models,\n",
    "                                                               metric=main_scoring_metric,\n",
    "                                                               by_partition=partition,\n",
    "                                                               feature_list_name=start_featurelist_name,\n",
    "                                                               ratio=ratio,\n",
    "                                                               model_search_params=best_model_search_params,\n",
    "                                                               use_ranks=use_ranks\n",
    "                                                              )\n",
    "        except dr.errors.ClientError as e:\n",
    "            #decay the ratio\n",
    "            ratio *= ratio\n",
    "            print(e, f'\\nWill try again with a ratio decay ...  New ratio={ratio:.3f}')\n",
    "            continue\n",
    "        \n",
    "        ##############################\n",
    "        ##### GET NEW BEST MODEL #####\n",
    "        ##############################\n",
    "        \n",
    "        new_best_model = get_best_models(project,\n",
    "                                         metric=main_scoring_metric, \n",
    "                                         by_partition = partition, \n",
    "                                         model_search_params=model_search_params).values[0]\n",
    "\n",
    "        \n",
    "        #################################\n",
    "        ##### PROSESS STOP CRITERIA #####\n",
    "        #################################\n",
    "        \n",
    "        if best_model.id == new_best_model.id:\n",
    "            #if no better model is produced with a recent run, burn 1 life\n",
    "            lifes -= 1\n",
    "            \n",
    "            #if no lifes left -> stop\n",
    "            if lifes <= 0:\n",
    "                print('New model is worse. No lifes left.\\nAUTOMATIC FEATURE SELECTION PROCESS HAS BEEN STOPPED')\n",
    "                return new_best_model\n",
    "            \n",
    "            #decay the ratio\n",
    "            ratio *= ratio\n",
    "            print(f'New model is worse. One life is burnt.\\nRepeat again with decaying the cumulative impact ratio. New ratio={ratio:.3f}')\n",
    "        \n",
    "        runs += 1\n",
    "        print('Run ', runs, ' completed')\n",
    "    \n",
    "    return new_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DR project and kick-off AutoPilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autopilot running for 300 seconds... Counter({'inprogress': 8})\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 20, 'queue': 12})\n",
      "Autopilot running for 900 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 1200 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 1500 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 1800 seconds... Counter({'inprogress': 4})\n",
      "Autopilot running for 2100 seconds... Counter({'inprogress': 2})\n",
      "Autopilot running for 2400 seconds... Counter()\n",
      "61a936fd1283f8039b8bbe7f\n"
     ]
    }
   ],
   "source": [
    "project = dr.Project.create('https://s3.amazonaws.com/datarobot_public_datasets/madelon_combined_80.csv')\n",
    "project.set_target(target='y',\n",
    "                   mode=dr.AUTOPILOT_MODE.FULL_AUTO,\n",
    "                   worker_count=-1,\n",
    "                  )\n",
    "\n",
    "def wait_for_autopilot(proj, check_interval=60*5):\n",
    "  total_wait = 0\n",
    "  while proj.get_status()['autopilot_done'] == False:\n",
    "    sleep(check_interval)\n",
    "    total_wait += check_interval\n",
    "    jobs = proj.get_model_jobs()\n",
    "    print(f'Autopilot running for {total_wait} seconds...', Counter(job.status for job in jobs))\n",
    "\n",
    "wait_for_autopilot(project)\n",
    "print(project.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once Project's inital AutoPilot is finished, let's start performing feature selection.<br>\n",
    "\n",
    "We start new Autopilot on feature list based on median rank aggregation of feature impacts across top 5 models trained on \"Informative Features\" list.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Feature Impact calculations\n",
      "Starting AutoPilot on a reduced feature list\n",
      "Autopilot running for 300 seconds... Counter({'inprogress': 8})\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 11})\n",
      "Autopilot running for 900 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 1200 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 1500 seconds... Counter()\n",
      "... AutoPilot is completed.\n",
      "Run  1  completed\n",
      "Request Feature Impact calculations\n",
      "Starting AutoPilot on a reduced feature list\n",
      "Autopilot running for 300 seconds... Counter({'inprogress': 8})\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 18, 'queue': 2})\n",
      "Autopilot running for 900 seconds... Counter({'inprogress': 4})\n",
      "Autopilot running for 1200 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 1500 seconds... Counter()\n",
      "... AutoPilot is completed.\n",
      "Run  2  completed\n",
      "Request Feature Impact calculations\n",
      "Starting AutoPilot on a reduced feature list\n",
      "Autopilot running for 300 seconds... Counter({'inprogress': 8})\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 18, 'queue': 1})\n",
      "Autopilot running for 900 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 1200 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 1500 seconds... Counter()\n",
      "Autopilot running for 1800 seconds... Counter()\n",
      "... AutoPilot is completed.\n",
      "Run  3  completed\n",
      "Request Feature Impact calculations\n",
      "Starting AutoPilot on a reduced feature list\n",
      "Autopilot running for 300 seconds... Counter()\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 20, 'queue': 9})\n",
      "Autopilot running for 900 seconds... Counter({'inprogress': 3})\n",
      "Autopilot running for 1200 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 1500 seconds... Counter()\n",
      "Autopilot running for 1800 seconds... Counter()\n",
      "... AutoPilot is completed.\n",
      "Run  4  completed\n",
      "Request Feature Impact calculations\n",
      "Starting AutoPilot on a reduced feature list\n",
      "Autopilot running for 300 seconds... Counter({'queue': 5})\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 5})\n",
      "Autopilot running for 900 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 1200 seconds... Counter()\n",
      "... AutoPilot is completed.\n",
      "Run  5  completed\n",
      "Request Feature Impact calculations\n",
      "Starting AutoPilot on a reduced feature list\n",
      "Autopilot running for 300 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 20, 'queue': 12})\n",
      "Autopilot running for 900 seconds... Counter({'inprogress': 5})\n",
      "Autopilot running for 1200 seconds... Counter()\n",
      "Autopilot running for 1500 seconds... Counter()\n",
      "... AutoPilot is completed.\n",
      "Run  6  completed\n",
      "Request Feature Impact calculations\n",
      "Starting AutoPilot on a reduced feature list\n",
      "Autopilot running for 300 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 13})\n",
      "Autopilot running for 900 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 1200 seconds... Counter()\n",
      "... AutoPilot is completed.\n",
      "Run  7  completed\n",
      "Request Feature Impact calculations\n",
      "Starting AutoPilot on a reduced feature list\n",
      "Autopilot running for 300 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 17, 'queue': 1})\n",
      "Autopilot running for 900 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 1200 seconds... Counter()\n",
      "... AutoPilot is completed.\n",
      "Run  8  completed\n",
      "Request Feature Impact calculations\n",
      "Starting AutoPilot on a reduced feature list\n",
      "Autopilot running for 300 seconds... Counter()\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 3})\n",
      "Autopilot running for 900 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 1200 seconds... Counter()\n",
      "... AutoPilot is completed.\n",
      "Run  9  completed\n",
      "Request Feature Impact calculations\n",
      "Starting AutoPilot on a reduced feature list\n",
      "Autopilot running for 300 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 9})\n",
      "Autopilot running for 900 seconds... Counter()\n",
      "Autopilot running for 1200 seconds... Counter()\n",
      "Autopilot running for 1500 seconds... Counter()\n",
      "... AutoPilot is completed.\n",
      "Run  10  completed\n",
      "Request Feature Impact calculations\n",
      "Starting AutoPilot on a reduced feature list\n",
      "Autopilot running for 300 seconds... Counter()\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 900 seconds... Counter()\n",
      "Autopilot running for 1200 seconds... Counter()\n",
      "... AutoPilot is completed.\n",
      "Run  11  completed\n",
      "Request Feature Impact calculations\n",
      "Starting AutoPilot on a reduced feature list\n",
      "Autopilot running for 300 seconds... Counter()\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 900 seconds... Counter()\n",
      "Autopilot running for 1200 seconds... Counter()\n",
      "... AutoPilot is completed.\n",
      "Run  12  completed\n",
      "Request Feature Impact calculations\n",
      "Starting AutoPilot on a reduced feature list\n",
      "Autopilot running for 300 seconds... Counter({'inprogress': 3})\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 13})\n",
      "Autopilot running for 900 seconds... Counter()\n",
      "... AutoPilot is completed.\n",
      "New model is worse. One life is burnt.\n",
      "Repeat again with decaying the cumulative impact ratio. New ratio=0.902\n",
      "Run  13  completed\n",
      "Request Feature Impact calculations\n",
      "Starting AutoPilot on a reduced feature list\n",
      "Autopilot running for 300 seconds... Counter()\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 2})\n",
      "Autopilot running for 900 seconds... Counter()\n",
      "Autopilot running for 1200 seconds... Counter()\n",
      "... AutoPilot is completed.\n",
      "Run  14  completed\n",
      "Request Feature Impact calculations\n",
      "Starting AutoPilot on a reduced feature list\n",
      "Autopilot running for 300 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 3})\n",
      "Autopilot running for 900 seconds... Counter()\n",
      "... AutoPilot is completed.\n",
      "New model is worse. One life is burnt.\n",
      "Repeat again with decaying the cumulative impact ratio. New ratio=0.815\n",
      "Run  15  completed\n",
      "Request Feature Impact calculations\n",
      "Starting AutoPilot on a reduced feature list\n",
      "Autopilot running for 300 seconds... Counter({'inprogress': 1})\n",
      "Autopilot running for 600 seconds... Counter({'inprogress': 3})\n",
      "Autopilot running for 900 seconds... Counter()\n",
      "... AutoPilot is completed.\n",
      "New model is worse. No lifes left.\n",
      "AUTOMATIC FEATURE SELECTION PROCESS HAS BEEN STOPPED\n"
     ]
    }
   ],
   "source": [
    "#feel free adjust function's parameters for your purposes. \n",
    "best_model = main_feature_selection(project.id,\n",
    "                                    partition='crossValidation',\n",
    "                                    best_model_search_params={'sample_pct__lte': 65})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has LogLoss score = 0.26367 on the cross-validation partition on the list of 16 features\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best model has {project.metric} score = {best_model.metrics[project.metric]['crossValidation']} on the cross-validation partition \\\n",
    "on the list of {len(best_model.get_features_used())} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
