{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Predictions and Prediction Explanations\n",
    "\n",
    "**Author**: Thodoris Petropoulos\n",
    "\n",
    "**Label**: Model Deployment\n",
    "### Scope\n",
    "\n",
    "The scope of this notebook is to provide instructions on how to get predictions and prediction explanations out of a trained model using the Python API.\n",
    "\n",
    "### Background\n",
    "\n",
    "The main ways you can get predictions out of DataRobot using Python would be the modeling API and the prediction API.\n",
    "\n",
    "**Modeling API**: You can use the modelling API if you use Python or R and there are multiple ways you can interact with it.\n",
    "\n",
    "**Prediction API**: Any project can be called with the Prediction API if you have prediction servers. This is a simple REST API. Click on a model in the UI, then \"Deploy Model\" and \"Activate now\". You'll have access to a Python code snippet to help you interact with it. You can also deploy the model through the python API.\n",
    "\n",
    "\n",
    "For the purposes of this tutorial, we will focus on the Modeling API. Note that this particular method of scoring utilizes modeling workers. This means that if someone is using these workers for modeling, your prediction is going to have to wait. This method of scoring is good for testing but not for deployment. For actual deployment, please deploy the model as a REST API through DataRobot's UI or through the API.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Python version 3.7.3\n",
    "-  DataRobot API version 2.19.0. \n",
    "Small adjustments might be needed depending on the Python version and DataRobot API version you are using.\n",
    "\n",
    "Full documentation of the Python package can be found here:Â https://datarobot-public-api-client.readthedocs-hosted.com\n",
    "\n",
    "It is assumed you already have a DataRobot <code>Project</code> object and a DataRobot <code>Model </code> object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datarobot as dr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requesting Predictions\n",
    "\n",
    "Before actually requesting predictions, you should upload the dataset you wish to predict via <code>Project.upload_dataset</code>. Previously uploaded datasets can be seen under <code>Project.get_datasets</code>. When uploading the dataset you can provide the path to a local file, a file object, raw file content, a pandas.DataFrame object, or the url to a publicly available dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploading prediction dataset\n",
    "dataset_from_path = project.upload_dataset('path/file')\n",
    "\n",
    "#Request predictions\n",
    "predict_job = model.request_predictions(dataset_from_path.id)\n",
    "\n",
    "#Waiting for prediction calculations\n",
    "predictions = predict_job.get_result_when_complete()\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requesting Prediction Explanations\n",
    "In order to create PredictionExplanations for a particular model and dataset, you must first Compute feature impact for the model via <code>dr.Model.get_or_request_feature_impact()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_or_request_feature_impact()\n",
    "\n",
    "pei = dr.PredictionExplanationsInitialization.create(project.id, model.id)\n",
    "\n",
    "#Wait for results of Prediction Explanations\n",
    "pei.get_result_when_complete()\n",
    "\n",
    "pe_job = dr.PredictionExplanations.create(project.id, model.id,  dataset_from_path.id)\n",
    "\n",
    "#Waiting for Job to Complete\n",
    "pe = pe_job.get_result_when_complete()\n",
    "\n",
    "df_pe = pe.get_all_as_dataframe()\n",
    "df_pe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Series Projects Caveats\n",
    "Prediction datasets are uploaded as normal predictions. However, when uploading a prediction dataset, a new parameter forecastPoint can be specified. The forecast point of a prediction dataset identifies the point in time relative which predictions should be generated, and if one is not specified when uploading a dataset, the server will choose the most recent possible forecast point. The forecast window specified when setting the partitioning options for the project determines how far into the future from the forecast point predictions should be calculated.\n",
    "\n",
    "**Important Note**:\n",
    "When uploading a dataset for Time Series projects scoring, you need to include the actual values from previous dates depending on the feature derivation setup. For example, if feature derivation window is -10 to -1 days and you want to forecast sales for the next 3 days, your dataset would look like this:\n",
    "\n",
    "| date       | sales | Known_in_advance_feature |\n",
    "|------------|-------|--------------------------|\n",
    "| 01/01/2019 | 130   | AAA                      |\n",
    "| 02/01/2019 | 123   | VVV                      |\n",
    "| 03/01/2019 | 412   | BBB                      |\n",
    "| 04/01/2019 | 321   | DDD                      |\n",
    "| 05/01/2019 | 512   | DDD                      |\n",
    "| 06/01/2019 | 623   | VVV                      |\n",
    "| 07/01/2019 | 356   | CCC                      |\n",
    "| 08/01/2019 | 133   | AAA                      |\n",
    "| 09/01/2019 | 356   | CCC                      |\n",
    "| 10/01/2019 | 654   | DDD                      |\n",
    "| 11/01/2019 |       | BBB                      |\n",
    "| 12/01/2019 |       | CCC                      |\n",
    "| 13/01/2019 |       | DDD                      |\n",
    "\n",
    "DataRobot will detect your forecast point as 10/01/2019 and then it will calculate lag features and make predictions for the missing dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Predictions from a DataRobot Deployment\n",
    "If you have used MLOps to deploy a model (DataRobot or Custom), you will have access to an API which you can call using an API Client. Below is a python script of an API Client. You can create your own API Client in the language of your choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Usage:\n",
    "    python datarobot-predict.py <input-file.csv>\n",
    " \n",
    "This example uses the requests library which you can install with:\n",
    "    pip install requests\n",
    "We highly recommend that you update SSL certificates with:\n",
    "    pip install -U urllib3[secure] certifi\n",
    "\"\"\"\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    " \n",
    "API_URL = 'Find this in Deployment -> Overview -> Summary -> Endpoint'\n",
    "API_KEY = 'YOUR_API_KEY'\n",
    "DATAROBOT_KEY = 'Find this in Deployment -> Predictions -> Prediction API -> Single mode -> on top of the code sample'\n",
    " \n",
    "DEPLOYMENT_ID = 'YOUR_DEPLOYMENT_ID'\n",
    "MAX_PREDICTION_FILE_SIZE_BYTES = 52428800  # 50 MB\n",
    " \n",
    " \n",
    "class DataRobotPredictionError(Exception):\n",
    "    \"\"\"Raised if there are issues getting predictions from DataRobot\"\"\"\n",
    " \n",
    " \n",
    "def make_datarobot_deployment_predictions(data, deployment_id):\n",
    "    \"\"\"\n",
    "    Make predictions on data provided using DataRobot deployment_id provided.\n",
    "    See docs for details:\n",
    "         https://app.eu.datarobot.com/docs/users-guide/predictions/api/new-prediction-api.html\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : str\n",
    "        Feature1,Feature2\n",
    "        numeric_value,string\n",
    "    deployment_id : str\n",
    "        The ID of the deployment to make predictions with.\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    Response schema:\n",
    "        https://app.eu.datarobot.com/docs/users-guide/predictions/api/new-prediction-api.html#response-schema\n",
    " \n",
    "    Raises\n",
    "    ------\n",
    "    DataRobotPredictionError if there are issues getting predictions from DataRobot\n",
    "    \"\"\"\n",
    "    # Set HTTP headers. The charset should match the contents of the file.\n",
    "    headers = {\n",
    "        'Content-Type': 'text/plain; charset=UTF-8',\n",
    "        'Authorization': 'Bearer {}'.format(API_KEY),\n",
    "        'DataRobot-Key': DATAROBOT_KEY,\n",
    "    }\n",
    " \n",
    "    url = API_URL.format(deployment_id=deployment_id)\n",
    "    # Make API request for predictions\n",
    "    predictions_response = requests.post(\n",
    "        url,\n",
    "        data=data,\n",
    "        headers=headers,\n",
    "    )\n",
    "    _raise_dataroboterror_for_status(predictions_response)\n",
    "    # Return a Python dict following the schema in the documentation\n",
    "    return predictions_response.json()\n",
    " \n",
    " \n",
    "def _raise_dataroboterror_for_status(response):\n",
    "    \"\"\"Raise DataRobotPredictionError if the request fails along with the response returned\"\"\"\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError:\n",
    "        err_msg = '{code} Error: {msg}'.format(\n",
    "            code=response.status_code, msg=response.text)\n",
    "        raise DataRobotPredictionError(err_msg)\n",
    " \n",
    " \n",
    "def main(filename, deployment_id):\n",
    "    \"\"\"\n",
    "    Return an exit code on script completion or error. Codes > 0 are errors to the shell.\n",
    "    Also useful as a usage demonstration of\n",
    "    `make_datarobot_deployment_predictions(data, deployment_id)`\n",
    "    \"\"\"\n",
    "    if not filename:\n",
    "        print(\n",
    "            'Input file is required argument. '\n",
    "            'Usage: python datarobot-predict.py <input-file.csv>')\n",
    "        return 1\n",
    "    data = open(filename, 'rb').read()\n",
    "    data_size = sys.getsizeof(data)\n",
    "    if data_size >= MAX_PREDICTION_FILE_SIZE_BYTES:\n",
    "        print(\n",
    "            'Input file is too large: {} bytes. '\n",
    "            'Max allowed size is: {} bytes.'\n",
    "        ).format(data_size, MAX_PREDICTION_FILE_SIZE_BYTES)\n",
    "        return 1\n",
    "    try:\n",
    "        predictions = make_datarobot_deployment_predictions(data, deployment_id)\n",
    "    except DataRobotPredictionError as exc:\n",
    "        print(exc)\n",
    "        return 1\n",
    "    print(json.dumps(predictions, indent=4))\n",
    "    return 0\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    filename = sys.argv[1]\n",
    "    sys.exit(main(filename, DEPLOYMENT_ID))\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
